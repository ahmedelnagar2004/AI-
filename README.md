# Project Examples: Search Algorithms in AI


## Overview

This document provides detailed examples of projects that teams can undertake for the Search Algorithms project. Each example includes a problem description, implementation approach, expected deliverables, and key considerations for applying the required search algorithms.

---

## Example 1: 8-Puzzle Problem

### Problem Description

The 8-puzzle is a sliding puzzle consisting of a 3×3 grid with 8 numbered tiles and one empty space. The objective is to arrange the tiles from an initial configuration to a goal configuration (typically with numbers 1-8 in order and the empty space in the bottom-right corner) by sliding tiles into the empty space.

### Why This Problem?

This is an excellent introductory problem because it has a well-defined state space, clear goal state, and allows straightforward application of multiple search algorithms. The problem is computationally tractable while still being challenging enough to demonstrate algorithm differences.

### Implementation Approach

**State Representation:** Each state can be represented as a tuple or list of 9 elements representing the grid configuration. For example, `(1, 2, 3, 4, 5, 6, 7, 8, 0)` represents the goal state where 0 is the empty space.

**Successor Function:** From any state, the successor function generates all valid moves by sliding a tile adjacent to the empty space into that space. A 3×3 grid typically has 2-4 possible moves depending on the empty space's position.

**Cost Function:** Each move has a uniform cost of 1.

**Heuristic Functions (for A*):**

- Manhattan Distance: Sum of the distances each tile is from its goal position
- Misplaced Tiles: Count of tiles not in their goal position

### Algorithms to Implement

All required algorithms should be implemented:

- **BFS**: Guarantees shortest path but may use significant memory
- **DFS**: Memory efficient but may not find optimal solution
- **UCS**: Finds optimal path with uniform costs
- **IDS**: Combines BFS optimality with DFS memory efficiency
- **A* with Manhattan Distance**: Should significantly outperform uninformed search
- **Hill Climbing**: May get stuck in local optima

### Expected Results

Teams should demonstrate that A* with Manhattan distance heuristic significantly outperforms uninformed search methods in terms of nodes explored and computation time. A comparison table might show:

| Algorithm | Nodes Explored | Solution Length | Time (ms) |
| --------- | -------------- | --------------- | --------- |
| BFS       | 15,000+        | 20              | 250       |
| DFS       | 8,000          | 45              | 180       |
| A*        | 200            | 20              | 15        |

### Deliverables

- Working implementation of all six algorithms
- Visualization of the puzzle state and solution path
- Performance comparison report with multiple test cases
- Analysis of heuristic effectiveness

---

## Example 2: Traveling Salesman Problem (TSP)

### Problem Description

Given a set of cities and the distances between each pair, find the shortest route that visits each city exactly once and returns to the starting city. This is a classic NP-hard optimization problem.

### Why This Problem?

TSP demonstrates the challenge of optimization problems where the search space grows exponentially. It's ideal for comparing heuristic approaches and understanding when exact solutions become computationally infeasible.

### Implementation Approach

**State Representation:** A state can be represented as a permutation of cities, for example `(0, 2, 1, 3, 4, 0)` representing a tour starting and ending at city 0.

**Successor Function:** Generate all possible next cities that haven't been visited yet.

**Cost Function:** The total distance of the tour.

**Heuristic Functions (for A*):**

- Minimum Spanning Tree (MST) lower bound: Estimate remaining distance using MST of unvisited cities
- Nearest Neighbor heuristic: Estimate based on nearest unvisited city

### Algorithms to Implement

- **BFS/DFS**: Only practical for very small instances (< 10 cities)
- **UCS**: Finds optimal solution but computationally expensive
- **A* with MST heuristic**: Better pruning of search space
- **Hill Climbing**: Quick approximation, may not be optimal
- **Genetic Algorithm**: Effective for larger instances, demonstrates evolutionary approach

### Expected Results

For a 15-city problem:

| Algorithm         | Solution Quality | Computation Time |
| ----------------- | ---------------- | ---------------- |
| Brute Force (UCS) | Optimal          | 10+ seconds      |
| A* with MST       | Optimal          | 2-3 seconds      |
| Hill Climbing     | 10-20% worse     | < 100ms          |
| Genetic Algorithm | 5-15% worse      | 500-1000ms       |

### Deliverables

- Implementation of all algorithms
- City distance matrix and visualization
- Solution path visualization
- Performance analysis showing trade-offs between solution quality and computation time
- Comparison of heuristic effectiveness

---

## Example 3: N-Queens Problem

### Problem Description

Place N queens on an N×N chessboard such that no two queens threaten each other (no two queens on the same row, column, or diagonal). The classic version uses N=8.

### Why This Problem?

This is a constraint satisfaction problem that demonstrates how search algorithms handle constraints. It also shows the effectiveness of genetic algorithms for combinatorial problems.

### Implementation Approach

**State Representation:** A list of N integers where index i represents the column position of the queen in row i. For example, `[0, 4, 7, 5, 2, 6, 1, 3]` represents a valid 8-queens solution.

**Successor Function:** For uninformed search, generate all possible placements. For constraint-based approaches, only generate valid placements that don't violate constraints.

**Constraint Check:** Verify that no two queens attack each other.

**Heuristic Functions:**

- Number of conflicts: Count pairs of queens that attack each other
- Remaining conflicts: Estimate based on current conflicts

### Algorithms to Implement

- **DFS with Backtracking**: Efficient for constraint satisfaction
- **BFS**: Less efficient but demonstrates the approach
- **Hill Climbing**: May find local optima
- **Genetic Algorithm**: Effective for larger N values

### Expected Results

For 8-Queens:

| Algorithm         | Solutions Found | Computation Time |
| ----------------- | --------------- | ---------------- |
| DFS Backtracking  | All 92          | 50ms             |
| Hill Climbing     | 1-2             | 10ms             |
| Genetic Algorithm | Multiple        | 200-500ms        |

### Deliverables

- Implementation of multiple algorithms
- Visualization of board configurations
- Count of solutions found (for 8-queens, there are 92 unique solutions)
- Performance comparison
- Analysis of algorithm efficiency for different board sizes (N=4, 8, 12)

---

## Example 4: Knapsack Problem

### Problem Description

Given a set of items, each with a weight and value, determine the maximum value that can be obtained by selecting items such that the total weight does not exceed a given capacity.

### Why This Problem?

This is a classic optimization problem with real-world applications (resource allocation, budget planning). It demonstrates the difference between exact and approximate solutions.

### Implementation Approach

**State Representation:** A binary vector where each element indicates whether an item is included (1) or not (0). For example, `[1, 0, 1, 1, 0]` means items 0, 2, and 3 are selected.

**Successor Function:** Generate all possible combinations of items (or use incremental approaches).

**Cost Function:** Total value of selected items (to maximize).

**Heuristic Functions:**

- Value-to-weight ratio: Prioritize high-value, low-weight items
- Fractional knapsack bound: Upper bound assuming items can be fractionally included

### Algorithms to Implement

- **BFS/DFS**: Exhaustive search of all combinations
- **UCS**: Finds optimal solution
- **A* with value-to-weight heuristic**: Prunes search space effectively
- **Hill Climbing**: Quick approximation
- **Genetic Algorithm**: Good for large instances

### Expected Results

For a 20-item problem with capacity 50:

| Algorithm         | Solution Value | Optimality | Time (ms) |
| ----------------- | -------------- | ---------- | --------- |
| Exhaustive (UCS)  | 100            | 100%       | 500       |
| A*                | 100            | 100%       | 50        |
| Hill Climbing     | 95             | 95%        | 5         |
| Genetic Algorithm | 98             | 98%        | 100       |

### Deliverables

- Implementation of all algorithms
- Item database with weights and values
- Solution visualization (items selected)
- Performance analysis
- Trade-off analysis between solution quality and computation time

---

## Example 5: Maze Pathfinding

### Problem Description

Find the shortest path from a start position to a goal position in a maze. The maze is represented as a grid where some cells are walls and others are open paths.

### Why This Problem?

This is a practical problem with applications in robotics and game development. It clearly demonstrates the difference between uninformed and informed search algorithms.

### Implementation Approach

**State Representation:** A coordinate pair (x, y) representing the current position in the maze.

**Successor Function:** Generate all adjacent cells (up, down, left, right) that are not walls.

**Cost Function:** Each move has a cost of 1 (or variable costs for different terrain types).

**Heuristic Functions:**

- Euclidean distance to goal
- Manhattan distance to goal
- Diagonal distance to goal

### Algorithms to Implement

All required algorithms, with emphasis on:

- **BFS**: Guarantees shortest path
- **A* with Manhattan distance**: Should be most efficient
- **Hill Climbing**: May not find optimal path

### Expected Results

For a 50×50 maze with multiple obstacles:

| Algorithm     | Path Length | Nodes Explored | Time (ms) |
| ------------- | ----------- | -------------- | --------- |
| BFS           | 45          | 800            | 20        |
| A*            | 45          | 150            | 5         |
| Hill Climbing | 50          | 100            | 3         |

### Deliverables

- Maze generator or loader
- Visualization of maze with solution path
- Implementation of all algorithms
- Performance comparison
- Analysis of heuristic effectiveness

---

## Example 6: Graph Coloring Problem

### Problem Description

Color the vertices of a graph using the minimum number of colors such that no two adjacent vertices have the same color.

### Why This Problem?

This is a constraint satisfaction problem with applications in scheduling, register allocation, and frequency assignment. It demonstrates how search algorithms handle constraint propagation.

### Implementation Approach

**State Representation:** A list of color assignments for each vertex.

**Successor Function:** Assign a color to the next uncolored vertex.

**Constraint Check:** Verify that no two adjacent vertices have the same color.

**Heuristic Functions:**

- Number of conflicts: Count constraint violations
- Remaining constraints: Estimate based on uncolored vertices

### Algorithms to Implement

- **DFS with Backtracking**: Efficient for constraint satisfaction
- **Hill Climbing**: May find suboptimal colorings
- **Genetic Algorithm**: For larger graphs

### Expected Results

For a 20-vertex graph with various densities, teams should show how algorithm performance varies with graph structure.

### Deliverables

- Graph representation and visualization
- Implementation of multiple algorithms
- Comparison of chromatic numbers found
- Performance analysis
- Analysis of algorithm behavior on different graph types

---

## General Implementation Guidelines for All Examples

### Code Organization

```
project/
├── src/
│   ├── algorithms/
│   │   ├── bfs.py
│   │   ├── dfs.py
│   │   ├── ucs.py
│   │   ├── ids.py
│   │   ├── astar.py
│   │   ├── hill_climbing.py
│   │   └── genetic_algorithm.py
│   ├── problems/
│   │   ├── puzzle_8.py
│   │   ├── tsp.py
│   │   └── [other problems]
│   └── main.py
├── tests/
│   └── test_algorithms.py
├── results/
│   └── [performance data and visualizations]
└── README.md
```

### Performance Metrics to Track

For each algorithm, teams should measure and report:

- **Time Complexity**: Wall-clock execution time
- **Space Complexity**: Peak memory usage
- **Solution Quality**: Optimality of solution found
- **Nodes Explored**: Number of nodes expanded during search
- **Path Cost**: Total cost of the solution path

### Visualization Recommendations

- Show the search tree or graph being explored
- Animate the solution path
- Display performance comparisons in bar charts or tables
- Include heatmaps showing algorithm efficiency across different problem sizes

### Report Structure

Each project report should include:

1. Problem description and formulation
2. Algorithm implementations and pseudocode
3. Heuristic design and justification (for A*)
4. Experimental methodology
5. Results and analysis
6. Conclusions and recommendations
7. References

---

## Choosing Your Project

When selecting a project, consider:

- **Complexity**: Choose a problem appropriately challenging for your team size and skill level
- **Interest**: Select a problem your team finds engaging
- **Feasibility**: Ensure the problem is solvable within the project timeline
- **Learning Value**: Pick a problem that will teach you about different algorithm characteristics

Teams are encouraged to discuss their project choice with the instructor during office hours to ensure it meets the course requirements.
